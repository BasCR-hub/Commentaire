{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préprocess au NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d’abord, install le paquet NLTK avec le gestionnaire de paquets + pip +:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk==3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ouizb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import twitter_samples\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Téléchargez des commentaires Tripadvisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>bonus_info</th>\n",
       "      <th>city</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Accueil sympa pas mal d’attente au final le ke...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Détails FOURCHETTE DE PRIX 2 895 AMD - 12 061 ...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>L’accueil est bonne mais les sandwichs mauvais...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Détails FOURCHETTE DE PRIX 2 895 AMD - 12 061 ...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>En balade dans le quartier nous cherchons un k...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Détails FOURCHETTE DE PRIX 2 895 AMD - 12 061 ...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>kebab identique à ailleurs cela reste un kebab...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Détails FOURCHETTE DE PRIX 2 895 AMD - 12 061 ...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Au moment où je rédige cet avis le restaurant ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Détails FOURCHETTE DE PRIX 2 895 AMD - 12 061 ...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13305</th>\n",
       "      <td>5</td>\n",
       "      <td>Ce n'est pas difficile face à l'Elysee vous re...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Détails CUISINES Française Européenne RÉGIMES ...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13306</th>\n",
       "      <td>7</td>\n",
       "      <td>Nous avons vraiment passé un excellent moment ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Détails CUISINES Française Européenne RÉGIMES ...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13307</th>\n",
       "      <td>3</td>\n",
       "      <td>Nous arrivons au service de 21h heure à laquel...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Détails FOURCHETTE DE PRIX 2 628 AMD - 13 140 ...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13308</th>\n",
       "      <td>1</td>\n",
       "      <td>Ici comme à l'Eden Roc ou dans un autre hotel ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Détails CUISINES Française Européenne RÉGIMES ...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13309</th>\n",
       "      <td>2</td>\n",
       "      <td>Excellente pizzaria. Un accueil très chaleureu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Emplacement et coordonnées 43 Rue de Charenton...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13310 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                            comment  rating  \\\n",
       "0               1  Accueil sympa pas mal d’attente au final le ke...     3.0   \n",
       "1               7  L’accueil est bonne mais les sandwichs mauvais...     1.0   \n",
       "2               1  En balade dans le quartier nous cherchons un k...     3.0   \n",
       "3               6  kebab identique à ailleurs cela reste un kebab...     3.0   \n",
       "4               5  Au moment où je rédige cet avis le restaurant ...     1.0   \n",
       "...           ...                                                ...     ...   \n",
       "13305           5  Ce n'est pas difficile face à l'Elysee vous re...     5.0   \n",
       "13306           7  Nous avons vraiment passé un excellent moment ...     5.0   \n",
       "13307           3  Nous arrivons au service de 21h heure à laquel...     5.0   \n",
       "13308           1  Ici comme à l'Eden Roc ou dans un autre hotel ...     5.0   \n",
       "13309           2  Excellente pizzaria. Un accueil très chaleureu...     5.0   \n",
       "\n",
       "                                              bonus_info   city sentiment  \n",
       "0      Détails FOURCHETTE DE PRIX 2 895 AMD - 12 061 ...  Paris   Neutral  \n",
       "1      Détails FOURCHETTE DE PRIX 2 895 AMD - 12 061 ...  Paris  Negative  \n",
       "2      Détails FOURCHETTE DE PRIX 2 895 AMD - 12 061 ...  Paris   Neutral  \n",
       "3      Détails FOURCHETTE DE PRIX 2 895 AMD - 12 061 ...  Paris   Neutral  \n",
       "4      Détails FOURCHETTE DE PRIX 2 895 AMD - 12 061 ...  Paris  Negative  \n",
       "...                                                  ...    ...       ...  \n",
       "13305  Détails CUISINES Française Européenne RÉGIMES ...  Paris  Positive  \n",
       "13306  Détails CUISINES Française Européenne RÉGIMES ...  Paris  Positive  \n",
       "13307  Détails FOURCHETTE DE PRIX 2 628 AMD - 13 140 ...  Paris  Positive  \n",
       "13308  Détails CUISINES Française Européenne RÉGIMES ...  Paris  Positive  \n",
       "13309  Emplacement et coordonnées 43 Rue de Charenton...  Paris  Positive  \n",
       "\n",
       "[13310 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('resampled_comments_2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On  a séparer les commentaires en fonction de leur note en 3 catégories:\n",
    "- supérieur ou égale à 4 => positif\n",
    "- égale à 3 => neutre\n",
    "- inférieur ou égale à 2 => négatif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je prends juste les commentaires, les notes  et les sentiments pour le moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accueil sympa pas mal d’attente au final le ke...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L’accueil est bonne mais les sandwichs mauvais...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>En balade dans le quartier nous cherchons un k...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kebab identique à ailleurs cela reste un kebab...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Au moment où je rédige cet avis le restaurant ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Comment reconnaître des avis laissés par un pa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ce n’est pas possible que cet endroit soit cla...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Très surpris du nombre de profil avec 1 seul c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pas de salle juste deux tabourets a la limite ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Je suis allée dans ce kebab après avoir vu les...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  rating sentiment\n",
       "0  Accueil sympa pas mal d’attente au final le ke...     3.0   Neutral\n",
       "1  L’accueil est bonne mais les sandwichs mauvais...     1.0  Negative\n",
       "2  En balade dans le quartier nous cherchons un k...     3.0   Neutral\n",
       "3  kebab identique à ailleurs cela reste un kebab...     3.0   Neutral\n",
       "4  Au moment où je rédige cet avis le restaurant ...     1.0  Negative\n",
       "5  Comment reconnaître des avis laissés par un pa...     1.0  Negative\n",
       "6  Ce n’est pas possible que cet endroit soit cla...     1.0  Negative\n",
       "7  Très surpris du nombre de profil avec 1 seul c...     1.0  Negative\n",
       "8  Pas de salle juste deux tabourets a la limite ...     1.0  Negative\n",
       "9  Je suis allée dans ce kebab après avoir vu les...     1.0  Negative"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = df[['comment', 'rating', 'sentiment']]\n",
    "comments.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suppression du bruit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On met tout le texte en minuscule avnat la tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accueil sympa pas mal d’attente au final le ke...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l’accueil est bonne mais les sandwichs mauvais...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en balade dans le quartier nous cherchons un k...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kebab identique à ailleurs cela reste un kebab...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>au moment où je rédige cet avis le restaurant ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13305</th>\n",
       "      <td>ce n'est pas difficile face à l'elysee vous re...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13306</th>\n",
       "      <td>nous avons vraiment passé un excellent moment ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13307</th>\n",
       "      <td>nous arrivons au service de 21h heure à laquel...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13308</th>\n",
       "      <td>ici comme à l'eden roc ou dans un autre hotel ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13309</th>\n",
       "      <td>excellente pizzaria. un accueil très chaleureu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13310 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  rating sentiment\n",
       "0      accueil sympa pas mal d’attente au final le ke...     3.0   Neutral\n",
       "1      l’accueil est bonne mais les sandwichs mauvais...     1.0  Negative\n",
       "2      en balade dans le quartier nous cherchons un k...     3.0   Neutral\n",
       "3      kebab identique à ailleurs cela reste un kebab...     3.0   Neutral\n",
       "4      au moment où je rédige cet avis le restaurant ...     1.0  Negative\n",
       "...                                                  ...     ...       ...\n",
       "13305  ce n'est pas difficile face à l'elysee vous re...     5.0  Positive\n",
       "13306  nous avons vraiment passé un excellent moment ...     5.0  Positive\n",
       "13307  nous arrivons au service de 21h heure à laquel...     5.0  Positive\n",
       "13308  ici comme à l'eden roc ou dans un autre hotel ...     5.0  Positive\n",
       "13309  excellente pizzaria. un accueil très chaleureu...     5.0  Positive\n",
       "\n",
       "[13310 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments['comment'] = comments['comment'].map(lambda x: x.lower())\n",
    "comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization des phrases en mot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "toknizer = RegexpTokenizer(r'''\\w'|\\w+|[^\\w\\s]''')\n",
    "token = comments.apply(lambda row: toknizer.tokenize(row['comment']),axis= 1)\n",
    "#token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppression de la ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re \n",
    "#string.punctuation\n",
    "token_1 = token.apply(lambda x: [item.strip(string.punctuation) for item in x])\n",
    "token_2 = token_1.apply(lambda x: [re.sub(r'\\’', '', item) for item in x])\n",
    "#token_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suppression des nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_4 = token_2.apply(lambda x: [re.sub(r'\\d', '', item) for item in x])\n",
    "\n",
    "#print(token_4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal : Suppression des mots vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ouizb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "0        [accueil, sympa, mal, , attente, final, kebab,...\n",
      "1        [, accueil, bonne, sandwichs, mauvais, plus, t...\n",
      "2        [balade, quartier, cherchons, kebab, voici, tr...\n",
      "3        [kebab, identique, ailleurs, cela, reste, keba...\n",
      "4        [moment, où, rédige, cet, avis, restaurant, cl...\n",
      "                               ...                        \n",
      "13305    [difficile, face, elysee, remontez, rue, côté,...\n",
      "13306    [vraiment, passé, excellent, moment, nouveau, ...\n",
      "13307    [arrivons, service, h, heure, laquelle, réserv...\n",
      "13308    [ici, comme, eden, roc, autre, hotel, groupe, ...\n",
      "13309    [excellente, pizzaria, , accueil, très, chaleu...\n",
      "Length: 13310, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('french')) \n",
    "stop_token = token_4.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "print(stop_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [accueil, symp, mal, , attent, final, kebab, c...\n",
      "1        [, accueil, bon, sandwich, mauv, plus, tromp, ...\n",
      "2        [balad, quarti, cherchon, kebab, voic, tripadv...\n",
      "3        [kebab, ident, ailleur, cel, rest, kebab, tran...\n",
      "4        [moment, où, rédig, cet, avis, restaur, class,...\n",
      "                               ...                        \n",
      "13305    [difficil, fac, elyse, remont, ru, côt, bristo...\n",
      "13306    [vrai, pass, excellent, moment, nouveau, resta...\n",
      "13307    [arrivon, servic, h, heur, laquel, réserv, dîn...\n",
      "13308    [ici, comm, eden, roc, autr, hotel, group, lev...\n",
      "13309    [excellent, pizzari, , accueil, tres, chaleur,...\n",
      "Length: 13310, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='french')\n",
    "stemm = stop_token.apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "print(stemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [accueil, symp, mal, , attent, final, kebab, c...\n",
       "1        [, accueil, bon, sandwich, mauv, plus, tromp, ...\n",
       "2        [balad, quarti, cherchon, kebab, voic, tripadv...\n",
       "3        [kebab, ident, ailleur, cel, rest, kebab, tran...\n",
       "4        [moment, où, rédig, cet, avis, restaur, class,...\n",
       "                               ...                        \n",
       "13305    [difficil, fac, elyse, remont, ru, côt, bristo...\n",
       "13306    [vrai, pass, excellent, moment, nouveau, resta...\n",
       "13307    [arrivon, servic, heure, heur, laquel, réserv,...\n",
       "13308    [ici, comm, eden, roc, autr, hotel, group, lev...\n",
       "13309    [excellent, pizzari, , accueil, tres, chaleur,...\n",
       "Length: 13310, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "lemmatizer = FrenchLefffLemmatizer()\n",
    "lemm = stemm.apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "lemm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, assemblons les tokens traités ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lemm)):\n",
    "    lemm[i] = ' '.join(lemm[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>lemmatiser_com</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accueil sympa pas mal d’attente au final le ke...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>accueil symp mal  attent final kebab correct s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l’accueil est bonne mais les sandwichs mauvais...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>accueil bon sandwich mauv plus tromp command ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en balade dans le quartier nous cherchons un k...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>balad quarti cherchon kebab voic tripadvisor i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kebab identique à ailleurs cela reste un kebab...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>kebab ident ailleur cel rest kebab transcend  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>au moment où je rédige cet avis le restaurant ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>moment où rédig cet avis restaur class eme tri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13305</th>\n",
       "      <td>ce n'est pas difficile face à l'elysee vous re...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>difficil fac elyse remont ru côt bristol  seco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13306</th>\n",
       "      <td>nous avons vraiment passé un excellent moment ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>vrai pass excellent moment nouveau restaur esp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13307</th>\n",
       "      <td>nous arrivons au service de 21h heure à laquel...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>arrivon servic heure heur laquel réserv dîn  a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13308</th>\n",
       "      <td>ici comme à l'eden roc ou dans un autre hotel ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>ici comm eden roc autr hotel group levur chimi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13309</th>\n",
       "      <td>excellente pizzaria. un accueil très chaleureu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>excellent pizzari  accueil tres chaleur pizz v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13310 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  rating sentiment  \\\n",
       "0      accueil sympa pas mal d’attente au final le ke...     3.0   Neutral   \n",
       "1      l’accueil est bonne mais les sandwichs mauvais...     1.0  Negative   \n",
       "2      en balade dans le quartier nous cherchons un k...     3.0   Neutral   \n",
       "3      kebab identique à ailleurs cela reste un kebab...     3.0   Neutral   \n",
       "4      au moment où je rédige cet avis le restaurant ...     1.0  Negative   \n",
       "...                                                  ...     ...       ...   \n",
       "13305  ce n'est pas difficile face à l'elysee vous re...     5.0  Positive   \n",
       "13306  nous avons vraiment passé un excellent moment ...     5.0  Positive   \n",
       "13307  nous arrivons au service de 21h heure à laquel...     5.0  Positive   \n",
       "13308  ici comme à l'eden roc ou dans un autre hotel ...     5.0  Positive   \n",
       "13309  excellente pizzaria. un accueil très chaleureu...     5.0  Positive   \n",
       "\n",
       "                                          lemmatiser_com  \n",
       "0      accueil symp mal  attent final kebab correct s...  \n",
       "1       accueil bon sandwich mauv plus tromp command ...  \n",
       "2      balad quarti cherchon kebab voic tripadvisor i...  \n",
       "3      kebab ident ailleur cel rest kebab transcend  ...  \n",
       "4      moment où rédig cet avis restaur class eme tri...  \n",
       "...                                                  ...  \n",
       "13305  difficil fac elyse remont ru côt bristol  seco...  \n",
       "13306  vrai pass excellent moment nouveau restaur esp...  \n",
       "13307  arrivon servic heure heur laquel réserv dîn  a...  \n",
       "13308  ici comm eden roc autr hotel group levur chimi...  \n",
       "13309  excellent pizzari  accueil tres chaleur pizz v...  \n",
       "\n",
       "[13310 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On rajoute à notre dataframe les données lemmatiser comme nouvelle colonne\n",
    "comments['lemmatiser_com'] = lemm\n",
    "comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fonctionnalités TF-IDF\n",
    "Cette méthode prend en compte non seulement l'occurrence d'un mot dans un seul commentaire (ou tweet) mais aussi dans le corpus entier.\n",
    "TF-IDF fonctionne en pénalisant les mots communs en leur attribuant des poids inférieurs tout en donnant de l'importance aux mots qui sont rares dans l'ensemble du corpus mais qui apparaissent en bon nombre dans peu de commentaires.\n",
    "\n",
    "Termes importants liés à TF-IDF:\n",
    "- TF = (Nombre de fois que le terme t apparaît dans un document) / (Nombre de termes dans le document)\n",
    "- IDF = log (N / n), où, N est le nombre de documents et n est le nombre de documents dans lesquels un terme t est apparu.\n",
    "- TF-IDF = TF * IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df = 0.90 , min_df = 2 , max_features = 1000 , stop_words = stop_words, ngram_range=(1, 2)) \n",
    "# TF-IDF matrice de fonctionnalités\n",
    "X = tfidf_vectorizer.fit_transform(comments['lemmatiser_com'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctionnalités Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words= stop_words)\n",
    "# bag-of-words matrice de fonctionnalités\n",
    "X_bow = bow_vectorizer.fit_transform(comments['lemmatiser_com'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction de modèles: analyse des sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = comments['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split trani et test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# TF-IDF matrice\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# bag-of-words matrice\n",
    "X_bow_train, X_bow_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79530516 0.81032864 0.8056338  0.79239079 0.79802724] Avg accuracy Random Forest avec TF-IDF matrice: 0.8003371284541444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# TF-IDF matrice\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "scores = cross_val_score(rf, X_train, y_train, cv=5)\n",
    "print(scores, \"Avg accuracy Random Forest avec TF-IDF matrice: {}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8        0.8084507  0.80187793 0.79051198 0.80178488] Avg accuracy Random Forest avec bag-of-words matrice: 0.8005250982960547\n"
     ]
    }
   ],
   "source": [
    "# bag-of-words matrice\n",
    "rf_2 = RandomForestClassifier()\n",
    "rf_2.fit(X_bow_train, y_train)\n",
    "scores_2 = cross_val_score(rf_2, X_bow_train, y_train, cv=5)\n",
    "print(scores_2, \"Avg accuracy Random Forest avec bag-of-words matrice: {}\".format(np.mean(scores_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Naive bayes gaussien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59624413 0.61643192 0.6056338  0.60216064 0.58900892] Avg accuracy GaussianNB: 0.6018958844660258\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "# TF-IDF matrice\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train.todense(), y_train) # il n'accepte pas les matrices clairsemées en entrée obliger de transformer en array numpy\n",
    "scores_gnb = cross_val_score(gnb, X_train.todense(), y_train, cv=5)\n",
    "print(scores_gnb, \"Avg accuracy GaussianNB: {}\".format(np.mean(scores_gnb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag-of-words matrice\n",
    "#gnb_2 = GaussianNB()\n",
    "#gnb_2.fit(X_bow_train.todense(), y_train) # il n'accepte pas les matrices clairsemées en entrée obliger de transformer en array numpy\n",
    "#scores_gnb2 = cross_val_score(gnb_2, X_bow_train.todense(), y_train, cv=5)\n",
    "#print(scores_gnb2, \"Avg accuracy GaussianNB: {}\".format(np.mean(scores_gnb2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Naive bayes multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70234742 0.70938967 0.71173709 0.70737435 0.7007985 ] Avg accuracy MultinomialNB avec TF-IDF matrice: 0.706329405901512\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# TF-IDF matrice\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train.todense(), y_train)\n",
    "scores_mnb = cross_val_score(mnb, X_train, y_train, cv=5)\n",
    "print(scores_mnb, \"Avg accuracy MultinomialNB avec TF-IDF matrice: {}\".format(np.mean(scores_mnb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71549296 0.71596244 0.70798122 0.70784406 0.70925317] Avg accuracy MultinomialNB avec bag-of-words matrice: 0.71130676969284\n"
     ]
    }
   ],
   "source": [
    "# bag-of-words matrice\n",
    "mnb2 = MultinomialNB()\n",
    "mnb2.fit(X_bow_train.todense(), y_train)\n",
    "scores_mnb2 = cross_val_score(mnb2, X_bow_train.todense(), y_train, cv=5)\n",
    "print(scores_mnb2, \"Avg accuracy MultinomialNB avec bag-of-words matrice: {}\".format(np.mean(scores_mnb2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Naive bayes complement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68967136 0.69201878 0.7        0.6787224  0.68670737] Avg accuracy ComplementNB avec TF-IDF matrice: 0.68942398401683\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "# TF-IDF matrice\n",
    "cnb = ComplementNB()\n",
    "cnb.fit(X_train, y_train)\n",
    "scores_cnb = cross_val_score(cnb, X_train, y_train, cv=5)\n",
    "print(scores_cnb, \"Avg accuracy ComplementNB avec TF-IDF matrice: {}\".format(np.mean(scores_cnb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag-of-words matrice\n",
    "#cnb2 = ComplementNB()\n",
    "#cnb2.fit(X_bow_train, y_train)\n",
    "#scores_cnb2 = cross_val_score(cnb2, X_bow_train, y_train, cv=5)\n",
    "#print(scores_cnb2, \"Avg accuracy ComplementNB avec bag-of-words matrice: {}\".format(np.mean(scores_cnb2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38920188 0.38920188 0.38873239 0.38891498 0.38938469] Avg accuracy SVM avec TF-IDF matrice: 0.38908716428837625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# TF-IDF matrice\n",
    "svc_model = SVC(gamma='auto')\n",
    "svc_model.fit(X_train, y_train)\n",
    "scores_svm = cross_val_score(svc_model, X_train, y_train, cv=5)\n",
    "print(scores_svm, \"Avg accuracy SVM avec TF-IDF matrice: {}\".format(np.mean(scores_svm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag-of-words matrice\n",
    "#svc_model2 = SVC(gamma='auto')\n",
    "#svc_model2.fit(X_bow_train, y_train)\n",
    "#scores_svm2 = cross_val_score(svc_model2, X_bow_train, y_train, cv=5)\n",
    "#print(scores_svm2, \"Avg accuracy SVM avec bag-of-words matrice: {}\".format(np.mean(scores_svm2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic régression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72535211 0.72910798 0.73802817 0.7242837  0.7350869 ] Avg accuracy LogisticRegression: 0.7303717718869975\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'warnings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-1ac258155b01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mscores_lg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogreg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores_lg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Avg accuracy LogisticRegression: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores_lg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'warnings' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# TF-IDF matrice\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "scores_lg = cross_val_score(logreg, X_train,y_train , cv=5)\n",
    "print(scores_lg, \"Avg accuracy LogisticRegression: {}\".format(np.mean(scores_lg)))\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73521127 0.72910798 0.72394366 0.71911696 0.72804133] Avg accuracy LogisticRegression: 0.7270842402150495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#bag-of-words matrice\n",
    "logreg2 = LogisticRegression()\n",
    "logreg2.fit(X_bow_train, y_train)\n",
    "scores_lg2 = cross_val_score(logreg2, X_bow_train, y_train, cv=5)\n",
    "print(scores_lg2, \"Avg accuracy LogisticRegression: {}\".format(np.mean(scores_lg2)))\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from sklearn import neighbors\n",
    "# TF-IDF matrice\n",
    "#knn = neighbors.KNeighborsClassifier(n_neighbors = 5,p=1,weights=\"distance\")\n",
    "#knn.fit(X_train, y_train)\n",
    "#scores_knn = cross_val_score(knn, X_train, y_train, cv=5)\n",
    "#print(scores_knn, \"Avg accuracy KNN avec TF-IDF matrice: {}\".format(np.mean(scores_knn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag-of-words matrice\n",
    "#knn2 = neighbors.KNeighborsClassifier(n_neighbors = 5,p=1,weights=\"distance\")\n",
    "#knn2.fit(X_bow_train, y_train)\n",
    "#scores_knn2 = cross_val_score(knn2, X_bow_train, y_train, cv=5)\n",
    "#print(scores_knn2, \"Avg accuracy KNN avec bag-of-words matrice: {}\".format(np.mean(scores_knn2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest à l'accuracy la plus élevée avec TF-IDF matrice avec 80.15%.\n",
    "\n",
    "Random Forest à l'accuracy la plus élevée avec bag-of-words matrice avec 80.24%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On teste nos prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_2.predict(X_bow_test) # RF avec bag-of-words matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[423 149  51]\n",
      " [ 41 934 103]\n",
      " [ 19 120 822]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.88      0.68      0.76       623\n",
      "     Neutral       0.78      0.87      0.82      1078\n",
      "    Positive       0.84      0.86      0.85       961\n",
      "\n",
      "    accuracy                           0.82      2662\n",
      "   macro avg       0.83      0.80      0.81      2662\n",
      "weighted avg       0.82      0.82      0.82      2662\n",
      "\n",
      "0.818557475582269\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Create regularization penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create regularization hyperparameter space\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "# Create grid search using 5-fold cross validation\n",
    "clf = GridSearchCV(logreg, hyperparameters, cv=5, verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model_lr = clf.fit(X, y)\n",
    "\n",
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model_lr.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model_lr.best_estimator_.get_params()['C'])\n",
    "\n",
    "# Predict target vector\n",
    "best_model_lr.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9627347858752817\n",
      "{'max_depth': 80, 'n_estimators': 100, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'n_estimators'      : [0.001, 0.01, 0.1,1,10,100, 100],\n",
    "    'max_depth'         : [ 60, 70, 80, 90, 100],\n",
    "    'random_state'      : [0],\n",
    "    #'max_features': ['auto'],\n",
    "    #'criterion' :['gini']\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(rf_2, parameters, cv=10, n_jobs=-1)\n",
    "clf.fit(X_bow_train, y_train)\n",
    "\n",
    "print(clf.score(X_bow, y))\n",
    "print(clf.best_params_)\n",
    "#print(classification_report(y_test, clf.predict(X_bow_test), digits=4))\n",
    "#print(zip(correct_train[predictor].columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7393688955672427\n",
      "{'alpha': 0.01}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.6050    0.5778    0.5911       623\n",
      "     Neutral     0.6733    0.6939    0.6834      1078\n",
      "    Positive     0.8253    0.8210    0.8232       961\n",
      "\n",
      "    accuracy                         0.7126      2662\n",
      "   macro avg     0.7012    0.6976    0.6992      2662\n",
      "weighted avg     0.7122    0.7126    0.7123      2662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "clf_nbm = GridSearchCV(mnb2, parameters, cv=10)\n",
    "clf_nbm.fit(X_bow_train, y_train)\n",
    "print(clf_nbm.score(X_bow, y))\n",
    "print(clf_nbm.best_params_)\n",
    "print(classification_report(y_test, clf_nbm.predict(X_bow_test), digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline avec tdifd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 315 candidates, totalling 630 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter criterion for estimator Pipeline(memory=None,\n         steps=[('tfidf',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=True, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words={'ai', 'aie', 'aient', 'aies',\n                                             'ait', 'as', 'au', 'aura', 'aurai',...\n                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n                                        class_weight=None, criterion='gini',\n                                        max_depth=None, max_features='auto',\n                                        max_leaf_nodes=None, max_samples=None,\n                                        min_impurity_decrease=0.0,\n                                        min_impurity_split=None,\n                                        min_samples_leaf=1, min_samples_split=2,\n                                        min_weight_fraction_leaf=0.0,\n                                        n_estimators=100, n_jobs=None,\n                                        oob_score=False, random_state=None,\n                                        verbose=0, warm_start=False))],\n         verbose=False). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 608, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in __call__\n    for func, args, kwargs in self.items]\n  File \"C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 504, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n  File \"C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 163, in set_params\n    self._set_params('steps', **kwargs)\n  File \"C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 50, in _set_params\n    super().set_params(**params)\n  File \"C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 236, in set_params\n    (key, self))\nValueError: Invalid parameter criterion for estimator Pipeline(memory=None,\n         steps=[('tfidf',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=True, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words={'ai', 'aie', 'aient', 'aies',\n                                             'ait', 'as', 'au', 'aura', 'aurai',...\n                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n                                        class_weight=None, criterion='gini',\n                                        max_depth=None, max_features='auto',\n                                        max_leaf_nodes=None, max_samples=None,\n                                        min_impurity_decrease=0.0,\n                                        min_impurity_split=None,\n                                        min_samples_leaf=1, min_samples_split=2,\n                                        min_weight_fraction_leaf=0.0,\n                                        n_estimators=100, n_jobs=None,\n                                        oob_score=False, random_state=None,\n                                        verbose=0, warm_start=False))],\n         verbose=False). Check the list of available parameters with `estimator.get_params().keys()`.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-fe77eb871a89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mgrid_search_tune\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mgrid_search_tune\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_bow_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best parameters set:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    708\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    687\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 689\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    907\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    910\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    561\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    433\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter criterion for estimator Pipeline(memory=None,\n         steps=[('tfidf',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=True, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words={'ai', 'aie', 'aient', 'aies',\n                                             'ait', 'as', 'au', 'aura', 'aurai',...\n                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n                                        class_weight=None, criterion='gini',\n                                        max_depth=None, max_features='auto',\n                                        max_leaf_nodes=None, max_samples=None,\n                                        min_impurity_decrease=0.0,\n                                        min_impurity_split=None,\n                                        min_samples_leaf=1, min_samples_split=2,\n                                        min_weight_fraction_leaf=0.0,\n                                        n_estimators=100, n_jobs=None,\n                                        oob_score=False, random_state=None,\n                                        verbose=0, warm_start=False))],\n         verbose=False). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer='word', binary=False,\n",
    "                                 decode_error='strict',\n",
    "                                 encoding='utf-8', input='content',\n",
    "                                 lowercase=True, max_df=1.0, max_features=None,\n",
    "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
    "                                 preprocessor=None, smooth_idf=True,stop_words=stop_words)),\n",
    "    ('clf', RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
    "                                        class_weight=None, criterion='gini',\n",
    "                                        max_depth=None, max_features='auto',\n",
    "                                        max_leaf_nodes=None, max_samples=None,\n",
    "                                        min_impurity_decrease=0.0,\n",
    "                                        min_impurity_split=None,\n",
    "                                        min_samples_leaf=1, min_samples_split=2,\n",
    "                                        min_weight_fraction_leaf=0.0,\n",
    "                                        n_estimators=100, n_jobs=None,\n",
    "                                        oob_score=False, random_state=None,\n",
    "                                        verbose=0, warm_start=False)),\n",
    "])\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'n_estimators'      : [0.001, 0.01, 0.1,1,10,100, 100],\n",
    "    'max_depth'         : [ 60, 70, 80, 90, 100],\n",
    "    'random_state'      : [0],\n",
    "    'max_features': ['auto'],\n",
    "    'criterion' :['gini']\n",
    "}\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=2, verbose=3)\n",
    "grid_search_tune.fit(X_bow_train, y_train)\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print(grid_search_tune.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste des nouveaux parametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7971831  0.80610329 0.80985915 0.79755754 0.80178488] Avg accuracy Random Forest avec bag-of-words matrice: 0.8024975908370215\n",
      "[[411 162  50]\n",
      " [ 46 933  99]\n",
      " [ 21 115 825]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.86      0.66      0.75       623\n",
      "     Neutral       0.77      0.87      0.82      1078\n",
      "    Positive       0.85      0.86      0.85       961\n",
      "\n",
      "    accuracy                           0.81      2662\n",
      "   macro avg       0.83      0.79      0.80      2662\n",
      "weighted avg       0.82      0.81      0.81      2662\n",
      "\n",
      "0.8148009015777611\n",
      "0.8148009015777611\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# bag-of-words matrice\n",
    "rf_3 = RandomForestClassifier(max_depth = 80, n_estimators = 100, random_state = 0)\n",
    "rf_3.fit(X_bow_train, y_train)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'final_model_malika.sav'\n",
    "pickle.dump(rf_3, open(filename, 'wb'))\n",
    "\n",
    "scores_3 = cross_val_score(rf_3, X_bow_train, y_train, cv=5)\n",
    "print(scores_3, \"Avg accuracy Random Forest avec bag-of-words matrice: {}\".format(np.mean(scores_3)))\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "y_pred = rf_3.predict(X_bow_test) # RF avec bag-of-words matrice\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_bow_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

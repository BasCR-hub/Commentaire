{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préprocess au NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d’abord, install le paquet NLTK avec le gestionnaire de paquets + pip +:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk==3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ouizb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import twitter_samples\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Téléchargez des commentaires Tripadvisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>bonus_info</th>\n",
       "      <th>city</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Accueil sympa pas mal d’attente au final le ke...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Détails FOURCHETTE DE PRIX 2 895 AMD - 12 061 ...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>L’accueil est bonne mais les sandwichs mauvais...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Détails FOURCHETTE DE PRIX 2 895 AMD - 12 061 ...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>En balade dans le quartier nous cherchons un k...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Détails FOURCHETTE DE PRIX 2 895 AMD - 12 061 ...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>kebab identique à ailleurs cela reste un kebab...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Détails FOURCHETTE DE PRIX 2 895 AMD - 12 061 ...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Au moment où je rédige cet avis le restaurant ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Détails FOURCHETTE DE PRIX 2 895 AMD - 12 061 ...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13305</th>\n",
       "      <td>5</td>\n",
       "      <td>Ce n'est pas difficile face à l'Elysee vous re...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Détails CUISINES Française Européenne RÉGIMES ...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13306</th>\n",
       "      <td>7</td>\n",
       "      <td>Nous avons vraiment passé un excellent moment ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Détails CUISINES Française Européenne RÉGIMES ...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13307</th>\n",
       "      <td>3</td>\n",
       "      <td>Nous arrivons au service de 21h heure à laquel...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Détails FOURCHETTE DE PRIX 2 628 AMD - 13 140 ...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13308</th>\n",
       "      <td>1</td>\n",
       "      <td>Ici comme à l'Eden Roc ou dans un autre hotel ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Détails CUISINES Française Européenne RÉGIMES ...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13309</th>\n",
       "      <td>2</td>\n",
       "      <td>Excellente pizzaria. Un accueil très chaleureu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Emplacement et coordonnées 43 Rue de Charenton...</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13310 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                            comment  rating  \\\n",
       "0               1  Accueil sympa pas mal d’attente au final le ke...     3.0   \n",
       "1               7  L’accueil est bonne mais les sandwichs mauvais...     1.0   \n",
       "2               1  En balade dans le quartier nous cherchons un k...     3.0   \n",
       "3               6  kebab identique à ailleurs cela reste un kebab...     3.0   \n",
       "4               5  Au moment où je rédige cet avis le restaurant ...     1.0   \n",
       "...           ...                                                ...     ...   \n",
       "13305           5  Ce n'est pas difficile face à l'Elysee vous re...     5.0   \n",
       "13306           7  Nous avons vraiment passé un excellent moment ...     5.0   \n",
       "13307           3  Nous arrivons au service de 21h heure à laquel...     5.0   \n",
       "13308           1  Ici comme à l'Eden Roc ou dans un autre hotel ...     5.0   \n",
       "13309           2  Excellente pizzaria. Un accueil très chaleureu...     5.0   \n",
       "\n",
       "                                              bonus_info   city sentiment  \n",
       "0      Détails FOURCHETTE DE PRIX 2 895 AMD - 12 061 ...  Paris   Neutral  \n",
       "1      Détails FOURCHETTE DE PRIX 2 895 AMD - 12 061 ...  Paris  Negative  \n",
       "2      Détails FOURCHETTE DE PRIX 2 895 AMD - 12 061 ...  Paris   Neutral  \n",
       "3      Détails FOURCHETTE DE PRIX 2 895 AMD - 12 061 ...  Paris   Neutral  \n",
       "4      Détails FOURCHETTE DE PRIX 2 895 AMD - 12 061 ...  Paris  Negative  \n",
       "...                                                  ...    ...       ...  \n",
       "13305  Détails CUISINES Française Européenne RÉGIMES ...  Paris  Positive  \n",
       "13306  Détails CUISINES Française Européenne RÉGIMES ...  Paris  Positive  \n",
       "13307  Détails FOURCHETTE DE PRIX 2 628 AMD - 13 140 ...  Paris  Positive  \n",
       "13308  Détails CUISINES Française Européenne RÉGIMES ...  Paris  Positive  \n",
       "13309  Emplacement et coordonnées 43 Rue de Charenton...  Paris  Positive  \n",
       "\n",
       "[13310 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('resampled_comments_2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On  a séparer les commentaires en fonction de leur note en 3 catégories:\n",
    "- supérieur ou égale à 4 => positif\n",
    "- égale à 3 => neutre\n",
    "- inférieur ou égale à 2 => négatif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je prends juste les commentaires, les notes  et les sentiments pour le moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accueil sympa pas mal d’attente au final le ke...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L’accueil est bonne mais les sandwichs mauvais...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>En balade dans le quartier nous cherchons un k...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kebab identique à ailleurs cela reste un kebab...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Au moment où je rédige cet avis le restaurant ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Comment reconnaître des avis laissés par un pa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ce n’est pas possible que cet endroit soit cla...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Très surpris du nombre de profil avec 1 seul c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pas de salle juste deux tabourets a la limite ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Je suis allée dans ce kebab après avoir vu les...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  rating sentiment\n",
       "0  Accueil sympa pas mal d’attente au final le ke...     3.0   Neutral\n",
       "1  L’accueil est bonne mais les sandwichs mauvais...     1.0  Negative\n",
       "2  En balade dans le quartier nous cherchons un k...     3.0   Neutral\n",
       "3  kebab identique à ailleurs cela reste un kebab...     3.0   Neutral\n",
       "4  Au moment où je rédige cet avis le restaurant ...     1.0  Negative\n",
       "5  Comment reconnaître des avis laissés par un pa...     1.0  Negative\n",
       "6  Ce n’est pas possible que cet endroit soit cla...     1.0  Negative\n",
       "7  Très surpris du nombre de profil avec 1 seul c...     1.0  Negative\n",
       "8  Pas de salle juste deux tabourets a la limite ...     1.0  Negative\n",
       "9  Je suis allée dans ce kebab après avoir vu les...     1.0  Negative"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = df[['comment', 'rating', 'sentiment']]\n",
    "comments.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suppression du bruit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On met tout le texte en minuscule avnat la tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accueil sympa pas mal d’attente au final le ke...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l’accueil est bonne mais les sandwichs mauvais...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en balade dans le quartier nous cherchons un k...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kebab identique à ailleurs cela reste un kebab...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>au moment où je rédige cet avis le restaurant ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13305</th>\n",
       "      <td>ce n'est pas difficile face à l'elysee vous re...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13306</th>\n",
       "      <td>nous avons vraiment passé un excellent moment ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13307</th>\n",
       "      <td>nous arrivons au service de 21h heure à laquel...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13308</th>\n",
       "      <td>ici comme à l'eden roc ou dans un autre hotel ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13309</th>\n",
       "      <td>excellente pizzaria. un accueil très chaleureu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13310 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  rating sentiment\n",
       "0      accueil sympa pas mal d’attente au final le ke...     3.0   Neutral\n",
       "1      l’accueil est bonne mais les sandwichs mauvais...     1.0  Negative\n",
       "2      en balade dans le quartier nous cherchons un k...     3.0   Neutral\n",
       "3      kebab identique à ailleurs cela reste un kebab...     3.0   Neutral\n",
       "4      au moment où je rédige cet avis le restaurant ...     1.0  Negative\n",
       "...                                                  ...     ...       ...\n",
       "13305  ce n'est pas difficile face à l'elysee vous re...     5.0  Positive\n",
       "13306  nous avons vraiment passé un excellent moment ...     5.0  Positive\n",
       "13307  nous arrivons au service de 21h heure à laquel...     5.0  Positive\n",
       "13308  ici comme à l'eden roc ou dans un autre hotel ...     5.0  Positive\n",
       "13309  excellente pizzaria. un accueil très chaleureu...     5.0  Positive\n",
       "\n",
       "[13310 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments['comment'] = comments['comment'].map(lambda x: x.lower())\n",
    "comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization des phrases en mot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "toknizer = RegexpTokenizer(r'''\\w'|\\w+|[^\\w\\s]''')\n",
    "token = comments.apply(lambda row: toknizer.tokenize(row['comment']),axis= 1)\n",
    "#token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppression de la ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re \n",
    "#string.punctuation\n",
    "token_1 = token.apply(lambda x: [item.strip(string.punctuation) for item in x])\n",
    "token_2 = token_1.apply(lambda x: [re.sub(r'\\’', '', item) for item in x])\n",
    "#token_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suppression des nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_4 = token_2.apply(lambda x: [re.sub(r'\\d', '', item) for item in x])\n",
    "\n",
    "#print(token_4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal : Suppression des mots vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ouizb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "0        [accueil, sympa, mal, , attente, final, kebab,...\n",
      "1        [, accueil, bonne, sandwichs, mauvais, plus, t...\n",
      "2        [balade, quartier, cherchons, kebab, voici, tr...\n",
      "3        [kebab, identique, ailleurs, cela, reste, keba...\n",
      "4        [moment, où, rédige, cet, avis, restaurant, cl...\n",
      "                               ...                        \n",
      "13305    [difficile, face, elysee, remontez, rue, côté,...\n",
      "13306    [vraiment, passé, excellent, moment, nouveau, ...\n",
      "13307    [arrivons, service, h, heure, laquelle, réserv...\n",
      "13308    [ici, comme, eden, roc, autre, hotel, groupe, ...\n",
      "13309    [excellente, pizzaria, , accueil, très, chaleu...\n",
      "Length: 13310, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('french')) \n",
    "stop_token = token_4.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "print(stop_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [accueil, symp, mal, , attent, final, kebab, c...\n",
      "1        [, accueil, bon, sandwich, mauv, plus, tromp, ...\n",
      "2        [balad, quarti, cherchon, kebab, voic, tripadv...\n",
      "3        [kebab, ident, ailleur, cel, rest, kebab, tran...\n",
      "4        [moment, où, rédig, cet, avis, restaur, class,...\n",
      "                               ...                        \n",
      "13305    [difficil, fac, elyse, remont, ru, côt, bristo...\n",
      "13306    [vrai, pass, excellent, moment, nouveau, resta...\n",
      "13307    [arrivon, servic, h, heur, laquel, réserv, dîn...\n",
      "13308    [ici, comm, eden, roc, autr, hotel, group, lev...\n",
      "13309    [excellent, pizzari, , accueil, tres, chaleur,...\n",
      "Length: 13310, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='french')\n",
    "stemm = stop_token.apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "print(stemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [accueil, symp, mal, , attent, final, kebab, c...\n",
       "1        [, accueil, bon, sandwich, mauv, plus, tromp, ...\n",
       "2        [balad, quarti, cherchon, kebab, voic, tripadv...\n",
       "3        [kebab, ident, ailleur, cel, rest, kebab, tran...\n",
       "4        [moment, où, rédig, cet, avis, restaur, class,...\n",
       "                               ...                        \n",
       "13305    [difficil, fac, elyse, remont, ru, côt, bristo...\n",
       "13306    [vrai, pass, excellent, moment, nouveau, resta...\n",
       "13307    [arrivon, servic, heure, heur, laquel, réserv,...\n",
       "13308    [ici, comm, eden, roc, autr, hotel, group, lev...\n",
       "13309    [excellent, pizzari, , accueil, tres, chaleur,...\n",
       "Length: 13310, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "lemmatizer = FrenchLefffLemmatizer()\n",
    "lemm = stemm.apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "lemm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, assemblons les tokens traités ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lemm)):\n",
    "    lemm[i] = ' '.join(lemm[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>lemmatiser_com</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accueil sympa pas mal d’attente au final le ke...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>accueil symp mal  attent final kebab correct s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>l’accueil est bonne mais les sandwichs mauvais...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>accueil bon sandwich mauv plus tromp command ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en balade dans le quartier nous cherchons un k...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>balad quarti cherchon kebab voic tripadvisor i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kebab identique à ailleurs cela reste un kebab...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>kebab ident ailleur cel rest kebab transcend  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>au moment où je rédige cet avis le restaurant ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Negative</td>\n",
       "      <td>moment où rédig cet avis restaur class eme tri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13305</th>\n",
       "      <td>ce n'est pas difficile face à l'elysee vous re...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>difficil fac elyse remont ru côt bristol  seco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13306</th>\n",
       "      <td>nous avons vraiment passé un excellent moment ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>vrai pass excellent moment nouveau restaur esp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13307</th>\n",
       "      <td>nous arrivons au service de 21h heure à laquel...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>arrivon servic heure heur laquel réserv dîn  a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13308</th>\n",
       "      <td>ici comme à l'eden roc ou dans un autre hotel ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>ici comm eden roc autr hotel group levur chimi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13309</th>\n",
       "      <td>excellente pizzaria. un accueil très chaleureu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>excellent pizzari  accueil tres chaleur pizz v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13310 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  rating sentiment  \\\n",
       "0      accueil sympa pas mal d’attente au final le ke...     3.0   Neutral   \n",
       "1      l’accueil est bonne mais les sandwichs mauvais...     1.0  Negative   \n",
       "2      en balade dans le quartier nous cherchons un k...     3.0   Neutral   \n",
       "3      kebab identique à ailleurs cela reste un kebab...     3.0   Neutral   \n",
       "4      au moment où je rédige cet avis le restaurant ...     1.0  Negative   \n",
       "...                                                  ...     ...       ...   \n",
       "13305  ce n'est pas difficile face à l'elysee vous re...     5.0  Positive   \n",
       "13306  nous avons vraiment passé un excellent moment ...     5.0  Positive   \n",
       "13307  nous arrivons au service de 21h heure à laquel...     5.0  Positive   \n",
       "13308  ici comme à l'eden roc ou dans un autre hotel ...     5.0  Positive   \n",
       "13309  excellente pizzaria. un accueil très chaleureu...     5.0  Positive   \n",
       "\n",
       "                                          lemmatiser_com  \n",
       "0      accueil symp mal  attent final kebab correct s...  \n",
       "1       accueil bon sandwich mauv plus tromp command ...  \n",
       "2      balad quarti cherchon kebab voic tripadvisor i...  \n",
       "3      kebab ident ailleur cel rest kebab transcend  ...  \n",
       "4      moment où rédig cet avis restaur class eme tri...  \n",
       "...                                                  ...  \n",
       "13305  difficil fac elyse remont ru côt bristol  seco...  \n",
       "13306  vrai pass excellent moment nouveau restaur esp...  \n",
       "13307  arrivon servic heure heur laquel réserv dîn  a...  \n",
       "13308  ici comm eden roc autr hotel group levur chimi...  \n",
       "13309  excellent pizzari  accueil tres chaleur pizz v...  \n",
       "\n",
       "[13310 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On rajoute à notre dataframe les données lemmatiser comme nouvelle colonne\n",
    "comments['lemmatiser_com'] = lemm\n",
    "comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fonctionnalités TF-IDF\n",
    "Cette méthode prend en compte non seulement l'occurrence d'un mot dans un seul commentaire (ou tweet) mais aussi dans le corpus entier.\n",
    "TF-IDF fonctionne en pénalisant les mots communs en leur attribuant des poids inférieurs tout en donnant de l'importance aux mots qui sont rares dans l'ensemble du corpus mais qui apparaissent en bon nombre dans peu de commentaires.\n",
    "\n",
    "Termes importants liés à TF-IDF:\n",
    "- TF = (Nombre de fois que le terme t apparaît dans un document) / (Nombre de termes dans le document)\n",
    "- IDF = log (N / n), où, N est le nombre de documents et n est le nombre de documents dans lesquels un terme t est apparu.\n",
    "- TF-IDF = TF * IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df = 0.90 , min_df = 2 , max_features = 1000 , stop_words = stop_words, ngram_range=(1, 2)) \n",
    "# TF-IDF matrice de fonctionnalités\n",
    "X = tfidf_vectorizer.fit_transform(comments['lemmatiser_com'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctionnalités Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words= stop_words)\n",
    "# bag-of-words matrice de fonctionnalités\n",
    "X_bow = bow_vectorizer.fit_transform(comments['lemmatiser_com'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction de modèles: analyse des sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = comments['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split trani et test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# TF-IDF matrice\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# bag-of-words matrice\n",
    "X_bow_train, X_bow_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80328638 0.80140845 0.8028169  0.79426961 0.80648192] Avg accuracy Random Forest avec TF-IDF matrice: 0.8016526527254964\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# TF-IDF matrice\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "scores = cross_val_score(rf, X_train, y_train, cv=5)\n",
    "print(scores, \"Avg accuracy Random Forest avec TF-IDF matrice: {}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79295775 0.80938967 0.81032864 0.79567872 0.80366369] Avg accuracy Random Forest avec bag-of-words matrice: 0.8024036941234065\n"
     ]
    }
   ],
   "source": [
    "# bag-of-words matrice\n",
    "rf_2 = RandomForestClassifier()\n",
    "rf_2.fit(X_bow_train, y_train)\n",
    "scores_2 = cross_val_score(rf_2, X_bow_train, y_train, cv=5)\n",
    "print(scores_2, \"Avg accuracy Random Forest avec bag-of-words matrice: {}\".format(np.mean(scores_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Naive bayes multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70234742 0.70938967 0.71173709 0.70737435 0.7007985 ] Avg accuracy MultinomialNB avec TF-IDF matrice: 0.706329405901512\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# TF-IDF matrice\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train.todense(), y_train)\n",
    "scores_mnb = cross_val_score(mnb, X_train, y_train, cv=5)\n",
    "print(scores_mnb, \"Avg accuracy MultinomialNB avec TF-IDF matrice: {}\".format(np.mean(scores_mnb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71549296 0.71596244 0.70798122 0.70784406 0.70925317] Avg accuracy MultinomialNB avec bag-of-words matrice: 0.71130676969284\n"
     ]
    }
   ],
   "source": [
    "# bag-of-words matrice\n",
    "mnb2 = MultinomialNB()\n",
    "mnb2.fit(X_bow_train.todense(), y_train)\n",
    "scores_mnb2 = cross_val_score(mnb2, X_bow_train.todense(), y_train, cv=5)\n",
    "print(scores_mnb2, \"Avg accuracy MultinomialNB avec bag-of-words matrice: {}\".format(np.mean(scores_mnb2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38920188 0.38920188 0.38873239 0.38891498 0.38938469] Avg accuracy SVM avec TF-IDF matrice: 0.38908716428837625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# TF-IDF matrice\n",
    "svc_model = SVC(gamma='auto')\n",
    "svc_model.fit(X_train, y_train)\n",
    "scores_svm = cross_val_score(svc_model, X_train, y_train, cv=5)\n",
    "print(scores_svm, \"Avg accuracy SVM avec TF-IDF matrice: {}\".format(np.mean(scores_svm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag-of-words matrice\n",
    "#svc_model2 = SVC(gamma='auto')\n",
    "#svc_model2.fit(X_bow_train, y_train)\n",
    "#scores_svm2 = cross_val_score(svc_model2, X_bow_train, y_train, cv=5)\n",
    "#print(scores_svm2, \"Avg accuracy SVM avec bag-of-words matrice: {}\".format(np.mean(scores_svm2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic régression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72535211 0.72910798 0.73802817 0.7242837  0.7350869 ] Avg accuracy LogisticRegression: 0.7303717718869975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# TF-IDF matrice\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "scores_lg = cross_val_score(logreg, X_train,y_train , cv=5)\n",
    "print(scores_lg, \"Avg accuracy LogisticRegression: {}\".format(np.mean(scores_lg)))\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73521127 0.72910798 0.72394366 0.71911696 0.72804133] Avg accuracy LogisticRegression: 0.7270842402150495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ouizb\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#bag-of-words matrice\n",
    "logreg2 = LogisticRegression()\n",
    "logreg2.fit(X_bow_train, y_train)\n",
    "scores_lg2 = cross_val_score(logreg2, X_bow_train, y_train, cv=5)\n",
    "print(scores_lg2, \"Avg accuracy LogisticRegression: {}\".format(np.mean(scores_lg2)))\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest à l'accuracy la plus élevée avec TF-IDF matrice avec 80.16%.\n",
    "\n",
    "Random Forest à l'accuracy la plus élevée avec bag-of-words matrice avec 80.24%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On teste nos prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_2.predict(X_bow_test) # RF avec bag-of-words matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[427 138  58]\n",
      " [ 45 935  98]\n",
      " [ 18 119 824]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.87      0.69      0.77       623\n",
      "     Neutral       0.78      0.87      0.82      1078\n",
      "    Positive       0.84      0.86      0.85       961\n",
      "\n",
      "    accuracy                           0.82      2662\n",
      "   macro avg       0.83      0.80      0.81      2662\n",
      "weighted avg       0.83      0.82      0.82      2662\n",
      "\n",
      "0.8211870773854245\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Create regularization penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create regularization hyperparameter space\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "# Create grid search using 5-fold cross validation\n",
    "clf = GridSearchCV(logreg, hyperparameters, cv=5, verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model_lr = clf.fit(X, y)\n",
    "\n",
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model_lr.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model_lr.best_estimator_.get_params()['C'])\n",
    "\n",
    "# Predict target vector\n",
    "best_model_lr.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9627347858752817\n",
      "{'max_depth': 80, 'n_estimators': 100, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'n_estimators'      : [0.001, 0.01, 0.1,1,10,100, 100],\n",
    "    'max_depth'         : [ 60, 70, 80, 90, 100],\n",
    "    'random_state'      : [0],\n",
    "    #'max_features': ['auto'],\n",
    "    #'criterion' :['gini']\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(rf_2, parameters, cv=10, n_jobs=-1)\n",
    "clf.fit(X_bow_train, y_train)\n",
    "\n",
    "print(clf.score(X_bow, y))\n",
    "print(clf.best_params_)\n",
    "#print(classification_report(y_test, clf.predict(X_bow_test), digits=4))\n",
    "#print(zip(correct_train[predictor].columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7393688955672427\n",
      "{'alpha': 0.01}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.6050    0.5778    0.5911       623\n",
      "     Neutral     0.6733    0.6939    0.6834      1078\n",
      "    Positive     0.8253    0.8210    0.8232       961\n",
      "\n",
      "    accuracy                         0.7126      2662\n",
      "   macro avg     0.7012    0.6976    0.6992      2662\n",
      "weighted avg     0.7122    0.7126    0.7123      2662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "clf_nbm = GridSearchCV(mnb2, parameters, cv=10)\n",
    "clf_nbm.fit(X_bow_train, y_train)\n",
    "print(clf_nbm.score(X_bow, y))\n",
    "print(clf_nbm.best_params_)\n",
    "print(classification_report(y_test, clf_nbm.predict(X_bow_test), digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste des nouveaux parametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7971831  0.80610329 0.80985915 0.79755754 0.80178488] Avg accuracy Random Forest avec bag-of-words matrice: 0.8024975908370215\n",
      "[[411 162  50]\n",
      " [ 46 933  99]\n",
      " [ 21 115 825]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.86      0.66      0.75       623\n",
      "     Neutral       0.77      0.87      0.82      1078\n",
      "    Positive       0.85      0.86      0.85       961\n",
      "\n",
      "    accuracy                           0.81      2662\n",
      "   macro avg       0.83      0.79      0.80      2662\n",
      "weighted avg       0.82      0.81      0.81      2662\n",
      "\n",
      "0.8148009015777611\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# bag-of-words matrice\n",
    "rf_3 = RandomForestClassifier(max_depth = 80, n_estimators = 100, random_state = 0)\n",
    "rf_3.fit(X_bow_train, y_train)\n",
    "\n",
    "# save the model to disk\n",
    "#filename = 'final_model_malika.sav'\n",
    "#pickle.dump(rf_3, open(filename, 'wb'))\n",
    "\n",
    "scores_3 = cross_val_score(rf_3, X_bow_train, y_train, cv=5)\n",
    "print(scores_3, \"Avg accuracy Random Forest avec bag-of-words matrice: {}\".format(np.mean(scores_3)))\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "y_pred = rf_3.predict(X_bow_test) # RF avec bag-of-words matrice\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# load the model from disk\n",
    "#loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_bow_test, y_test)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71455399 0.71549296 0.7084507  0.70878347 0.71395021] Avg accuracy MultinomialNB avec bag-of-words matrice: 0.71130676969284\n",
      "[[360 227  36]\n",
      " [199 748 131]\n",
      " [ 36 136 789]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.61      0.58      0.59       623\n",
      "     Neutral       0.67      0.69      0.68      1078\n",
      "    Positive       0.83      0.82      0.82       961\n",
      "\n",
      "    accuracy                           0.71      2662\n",
      "   macro avg       0.70      0.70      0.70      2662\n",
      "weighted avg       0.71      0.71      0.71      2662\n",
      "\n",
      "0.7126220886551465\n"
     ]
    }
   ],
   "source": [
    "# bag-of-words matrice\n",
    "mnb3 = MultinomialNB(alpha = 0.01)\n",
    "mnb3.fit(X_bow_train.todense(), y_train)\n",
    "scores_mnb3 = cross_val_score(mnb3, X_bow_train.todense(), y_train, cv=5)\n",
    "print(scores_mnb3, \"Avg accuracy MultinomialNB avec bag-of-words matrice: {}\".format(np.mean(scores_mnb2)))\n",
    "y_pred2 = mnb3.predict(X_bow_test) # NBM avec bag-of-words matrice\n",
    "print(confusion_matrix(y_test,y_pred2))\n",
    "print(classification_report(y_test,y_pred2))\n",
    "print(accuracy_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7595792637114951\n",
      "[[463 125  35]\n",
      " [104 877  97]\n",
      " [ 81 198 682]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "model_bag = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))\n",
    "model_bag.fit(X_train, y_train)\n",
    "model_bag.score(X_test,y_test)\n",
    "\n",
    "y_pred_bag = model_bag.predict(X_test)\n",
    "\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_bag))\n",
    "\n",
    "\n",
    "cnf_matrix_bag = confusion_matrix(y_test, y_pred_bag)\n",
    "print(cnf_matrix_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7573253193087904\n",
      "[[442 134  47]\n",
      " [102 873 103]\n",
      " [ 70 190 701]]\n"
     ]
    }
   ],
   "source": [
    "model_bag2 = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))\n",
    "model_bag2.fit(X_bow_train, y_train)\n",
    "model_bag2.score(X_bow_test,y_test)\n",
    "\n",
    "y_pred_bag2 = model_bag2.predict(X_bow_test)\n",
    "\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_bag2))\n",
    "\n",
    "\n",
    "cnf_matrix_bag2 = confusion_matrix(y_test, y_pred_bag2)\n",
    "print(cnf_matrix_bag2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6036814425244177\n",
      "[[285 255  83]\n",
      " [263 630 185]\n",
      " [ 80 189 692]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model_ada = AdaBoostClassifier(random_state=1)\n",
    "model_ada.fit(X_train, y_train)\n",
    "#model.score(x_test,y_test)\n",
    "\n",
    "y_pred_ada = model_ada.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_ada))\n",
    "\n",
    "\n",
    "cnf_matrix_ada = confusion_matrix(y_test, y_pred_ada)\n",
    "print(cnf_matrix_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36626596543951917\n",
      "[[ 88 355 180]\n",
      " [109 620 349]\n",
      " [113 581 267]]\n"
     ]
    }
   ],
   "source": [
    "model_ada2 = AdaBoostClassifier(random_state=1)\n",
    "model_ada2.fit(X_train, y_train)\n",
    "#model.score(x_test,y_test)\n",
    "\n",
    "y_pred_ada2 = model_ada.predict(X_bow_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_ada2))\n",
    "\n",
    "\n",
    "cnf_matrix_ada2 = confusion_matrix(y_test, y_pred_ada2)\n",
    "print(cnf_matrix_ada2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9941773102930128 Avg accuracy RandomForestClassifier: 0.9941773102930128\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "estimators = [('Random forest', rf), (\"Naive bayes\", mnb),(\"SVM\", svc_model)]\n",
    "reg = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier())\n",
    "reg.fit(X_train, y_train)\n",
    "scores_reg = reg.score(X_train, y_train)\n",
    "\n",
    "print(scores_reg, \"Avg accuracy RandomForestClassifier: {}\".format(np.mean(scores_reg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8395942900075132\n"
     ]
    }
   ],
   "source": [
    "y_pred_reg = reg.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[488 110  25]\n",
      " [101 911  66]\n",
      " [ 32  93 836]]\n"
     ]
    }
   ],
   "source": [
    "cnf_matrix_reg = confusion_matrix(y_test, y_pred_reg)\n",
    "print(cnf_matrix_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9973703981968445 Avg accuracy RandomForestClassifier: 0.9973703981968445\n"
     ]
    }
   ],
   "source": [
    "estimators = [('Random forest', rf), (\"Naive bayes\", mnb),(\"SVM\", svc_model)]\n",
    "reg2 = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier())\n",
    "reg2.fit(X_bow_train, y_train)\n",
    "scores_reg2 = reg2.score(X_bow_train, y_train)\n",
    "accu_reg2 = np.mean(scores_reg2)\n",
    "print(scores_reg2, \"Avg accuracy RandomForestClassifier: {}\".format(accu_reg2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37340345604808417\n"
     ]
    }
   ],
   "source": [
    "y_pred_reg2 = reg.predict(X_bow_test)\n",
    "print(accuracy_score(y_test, y_pred_reg2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9929564237415477 Avg accuracy RandomForestClassifier: 0.9929564237415477\n"
     ]
    }
   ],
   "source": [
    "estimators = [('Random forest', rf), (\"Naive bayes\", mnb),(\"SVM\", svc_model)]\n",
    "reg3 = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier(max_depth = 80, n_estimators = 100, random_state = 0))\n",
    "reg3.fit(X_train, y_train)\n",
    "scores_reg3 = reg3.score(X_train, y_train)\n",
    "\n",
    "print(scores_reg3, \"Avg accuracy RandomForestClassifier: {}\".format(np.mean(scores_reg3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8373403456048084\n"
     ]
    }
   ],
   "source": [
    "y_pred_reg3 = reg3.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('Random forest', rf), (\"Naive bayes\", mnb),(\"SVM\", svc_model)]\n",
    "reg3 = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier(max_depth = 80, n_estimators = 100, random_state = 0))\n",
    "reg3.fit(X_train, y_train)\n",
    "scores_reg3 = reg3.score(X_train, y_train)\n",
    "\n",
    "print(scores_reg3, \"Avg accuracy RandomForestClassifier: {}\".format(np.mean(scores_reg3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_reg3 = reg3.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_reg3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73004695 0.74553991 0.74976526 0.73790512 0.73790512] Avg accuracy Voting: 0.7402324704450279\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "voting = VotingClassifier(estimators=[('SVM', svc_model), ('Naive bayes', mnb),('Random forest', rf) ], voting='hard')\n",
    "voting.fit(X_train, y_train)\n",
    "scores_voting = cross_val_score(voting, X_train, y_train, cv=5)\n",
    "print(scores_voting, \"Avg accuracy Voting: {}\".format(np.mean(scores_voting)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7426746806912096\n"
     ]
    }
   ],
   "source": [
    "y_pred_voting = voting.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_voting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
